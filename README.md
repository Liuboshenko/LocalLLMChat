# LocalLLMChat
Web UI for local LLM inference. Run language models on your own machine with a clean chat interface. No cloud required - everything stays private and local.
